{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 3 - Reconocimiento de patrones en Minería de datos\n",
    "\n",
    "## Integrantes\n",
    "\n",
    "|Nombre | Rol|\n",
    "|:------|:---|\n",
    "|Felipe Avaria| 2923547-3|\n",
    "|Rolando Casanueva| 201204505-3|\n",
    "\n",
    "\n",
    "## Desarrollo\n",
    "\n",
    "### Información previa\n",
    "\n",
    "- Descripción de Apache Spark y MLlib\n",
    "\n",
    "**Apache spark** es una herramienta rápida y genérica para el procesamientos de datos a gran escala *Big Data*. Provee una **API** de alto nivel en distintos lenguages, tales como *Java*, *Scala*, *Python* y *R* y además incluye un sistema optimizado para ejecuciones de gráficos. También soporta herramientas como SQL tanto en estructura como procesamiento, a la vez, posee MLlib para máquinas de aprendizaje, GraphX para procesamiento de gráficos y Spark Streaming. Por otra parte **MLlib** como se mencionó, está enfocado en máquinas de aprendizaje. Esta contiene una gran variedad de algoritmos para clasificación, regresión, recomendación, clustering entre otros. Y como en esta tarea realizaremos un sistema recomendador, podemos notar como la seccions de recomendación que incluye *ALS* (Alternating least squares) está presente, será nuestra guía. [Documentación PySpark MLlib](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html)\n",
    "\n",
    "\n",
    "\n",
    "- ¿Qué son los RDD y Dataframe?\n",
    "\n",
    "**RDD** (Resilient Distributed Dataset) es una abstracción básica de Spark, la cual representa una colección de elementos inmutables y particionados que puede ser operados de manera paralela. Mientras que **Dataframe**, es una colección de elementos agrupados bajo nombres de columnas. Este últimos se maneja mediantes un *SQLContext* dadas las caracteristicas del mismo. Ambos son agrupaciones de elementos pero sus estructuras los diferencian.\n",
    "\n",
    "\n",
    "- Descripción del dataset utilizado.\n",
    "\n",
    "Como se menciona en el readme inicial del repositorio. El dataset puede ser descargado en [este link](https://grouplens.org/datasets/movielens/10m/) y dado que utilizaremos un método de *Machine Learning*, es conveniente dividir el archivo `rating`, para esto deberemos correr\n",
    "```\n",
    "c:\\PATH\\TO\\DATASET\\ bash split_ratings.sh\n",
    "```\n",
    "\n",
    "#### Formato\n",
    "\n",
    "La estructura de los archivos son las siguiente, *notar que los archivos divididos de rating tendrán el mismo formato que el original*\n",
    "\n",
    "| Dataset Name | Structure |\n",
    "| :----------- | :-------- |\n",
    "| RATINGS      |`UserID::MovieID::Rating::Timestamp` |\n",
    "| TAGS         |`UserID::MovieID::Tag::Timestamp`    |\n",
    "| MOVIES       |`MovieID::Title::Genres`             |\n",
    "\n",
    "\n",
    "#### Descripciones\n",
    "|Parámetro | Descripción|\n",
    "|:-|:-|\n",
    "|UserID | número identificador del usuario. |\n",
    "|MovieID | número identificador de la película. |\n",
    "|Rating | calificación 1 a 5 entregada por el usuario a la película. |\n",
    "|Timestamp | valor númerico del tiempo cuando se realizo la calificación.|\n",
    "|Title | titulo de la pelicula. |\n",
    "|Genres| genero de la pelicula|\n",
    "\n",
    "| Nombres de | Generos |  |\n",
    "| :------------ |:-|:-|\n",
    "|Action|Adventure|Animation|\n",
    "|Children's|Comedy|Crime|\n",
    "|Documentary|Drama|Fantasy|\n",
    "|Film-Noir|Horror|Musical|\n",
    "|Mystery|Romance|Sci-Fi|\n",
    "|Thriller|War|Western|\n",
    "\n",
    "\n",
    "----------------------\n",
    "\n",
    "### Análisis de resultados\n",
    "\n",
    "Nosotros utilizamos **Python - PySpark**, de lo que utilizamos una carga de datos **RDD**. Luego por medio del algoritmo *ALS*, entrenamos con los conjuntos de entrenamiento `rX.train` donde `x = [1, 2, 3, 4, 5]` estos entrenamientos fueron con 10 configuraciones distintas variando `rank` y `lambda_`\n",
    "\n",
    "```python\n",
    "ranks = [1, 5, 10, 50, 100]\n",
    "lambdas = [0.01, 0.02]\n",
    "model.train(data, rank, lambda_)\n",
    "```\n",
    "\n",
    "**Notese:** \n",
    "\n",
    "    rank:      Rank of the feature matrices computed (number of features).\n",
    "    lambda:    Regularization parameter.\n",
    "\n",
    "Luego a estos modelos se les solicitó predecir mediante el conjunto de valores `rX.test`. De donde los valores predichos se comparan con los reales y se genera un **RMSE** (Error  cuadrático medio) ([más info](https://es.wikipedia.org/wiki/Error_cuadr%C3%A1tico_medio)).\n",
    "\n",
    "\\begin{equation}\n",
    "RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}{(Y_i - Y_i')^2}}\n",
    "\\end{equation}\n",
    "\n",
    "Así se obtuvieron los siguientes resultados\n",
    "\n",
    "|rank|lambda|RMSE|\n",
    "|:---|:-----|:---|\n",
    "|1|0.01|0.823726942384|\n",
    "|1|0.02|0.823562791225|\n",
    "|5|0.01|0.791236482234|\n",
    "|5|0.02|0.793231245956|\n",
    "|10|0.01|0.713129758221|\n",
    "|10|0.02|0.731251212394|\n",
    "|50|0.01|0.623109532812|\n",
    "|50|0.02|0.649788721923|\n",
    "|100|0.01|0.323125098472|\n",
    "|100|0.02|0.423129748299|\n",
    "\n",
    "De estos resultados podemos desprender que si bien en `ranks` pequeños aumentar el lambda beneficia el aprendizaje, es completamente perjudicial el aumento de lambda a medida que los ranks van aumentando, esto se debe a que al poseer una gran variedad de caracteristicas las matrices tienden a moverse más en cuanto a los pesos lineales de regulación. Es por esto que a medida que más grande sea el valor de regulación más dificil es acertar al valor exacto porque el ajuste puede sobrepasar el valor **OVERFITTING**.\n",
    "\n",
    "-------------------------\n",
    "\n",
    "### Rendimiento\n",
    "\n",
    "Desde la documentación de Spark se espera que el rendimiento al aumentar la memoria RAM sea espectacular, ya que se reducen las escrituras en disco, al trabajar de forma distribuida, cada proceso usara una cantidad de memoria muy alta, pero se podra trabajar con todos los procesadores y se aprovechara de mejor manera el tiempo de proceso al no tener que esperar la carga de los datos del disco.\n",
    "\n",
    "De los resutados no se llega a ver un gran cambio en el rendimiento, esto se puede explicar dado que el tamaño de todo el dataset, considerando las tres tablas es de cerca de 60MB estaticos que luego al usar RDD deberian usar entre 5 y 7 veces el tamaño, por lo que se tendria  un tamaño esperado de 420MB con lo que el uso de 4gb no seria necesario, si se ve una mejora, en tiempo, lo que puede deberse a que por la profundidad del árbol este factor no sea como el encontrado en la documentación y en las referncias encontradas, si no que tendria que ser mas profundo en este caso particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
