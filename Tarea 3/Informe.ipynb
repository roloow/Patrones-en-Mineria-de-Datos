{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 3 - Reconocimiento de patrones en Minería de datos\n",
    "\n",
    "## Integrantes\n",
    "\n",
    "|Nombre | Rol|\n",
    "|:------|:---|\n",
    "|Felipe Avaria| 2923547-3|\n",
    "|Rolando Casanueva| 201204505-3|\n",
    "\n",
    "\n",
    "## Desarrollo\n",
    "\n",
    "### Información previa\n",
    "\n",
    "- Descripción de Apache Spark y MLlib\n",
    "\n",
    "**Apache spark** es una herramienta rápida y genérica para el procesamientos de datos a gran escala *Big Data*. Provee una **API** de alto nivel en distintos lenguages, tales como *Java*, *Scala*, *Python* y *R* y además incluye un sistema optimizado para ejecuciones de gráficos. También soporta herramientas como SQL tanto en estructura como procesamiento, a la vez, posee MLlib para máquinas de aprendizaje, GraphX para procesamiento de gráficos y Spark Streaming. Por otra parte **MLlib** como se mencionó, está enfocado en máquinas de aprendizaje. Esta contiene una gran variedad de algoritmos para clasificación, regresión, recomendación, clustering entre otros. Y como en esta tarea realizaremos un sistema recomendador, podemos notar como la seccions de recomendación que incluye *ALS* (Alternating least squares) está presente, será nuestra guía. [Documentación PySpark MLlib](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html)\n",
    "\n",
    "\n",
    "\n",
    "- ¿Qué son los RDD y Dataframe?\n",
    "\n",
    "**RDD** (Resilient Distributed Dataset) es una abstracción básica de Spark, la cual representa una colección de elementos inmutables y particionados que puede ser operados de manera paralela. Mientras que **Dataframe**, es una colección de elementos agrupados bajo nombres de columnas. Este últimos se maneja mediantes un *SQLContext* dadas las caracteristicas del mismo. Ambos son agrupaciones de elementos pero sus estructuras los diferencian.\n",
    "\n",
    "\n",
    "- Descripción del dataset utilizado.\n",
    "\n",
    "Como se menciona en el readme inicial del repositorio. El dataset puede ser descargado en [este link](https://grouplens.org/datasets/movielens/10m/) y dado que utilizaremos un método de *Machine Learning*, es conveniente dividir el archivo `rating`, para esto deberemos correr\n",
    "```\n",
    "c:\\PATH\\TO\\DATASET\\ bash split_ratings.sh\n",
    "```\n",
    "La estructura de los archivos son las siguiente, *notar que los archivos divididos de rating tendrán el mismo formato que el original*\n",
    "\n",
    "| Dataset Name | Structure |\n",
    "| :----------- | :-------- |\n",
    "| RATINGS      |`UserID::MovieID::Rating::Timestamp` |\n",
    "| TAGS         |`UserID::MovieID::Tag::Timestamp`    |\n",
    "| MOVIES       |`MovieID::Title::Genres`             |\n",
    "\n",
    "----------------------\n",
    "\n",
    "### Análisis de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
